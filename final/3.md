## Transformer Model Architecture
~~~
class EncoderDecoder(nn.Module):
"""
A standard Encoder-Decoder architecture. Base for this and many
other models.
"""
def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
super(EncoderDecoder, self).__init__()
self.encoder = encoder
self.decoder = decoder
self.src_embed = src_embed
self.tgt_embed = tgt_embed
self.generator = generator
def forward(self, src, tgt, src_mask, tgt_mask):
"Take in and process masked src and target sequences."
return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)
def encode(self, src, src_mask):
return self.encoder(self.src_embed(src), src_mask)
def decode(self, memory, src_mask, tgt, tgt_mask):
return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
class Generator(nn.Module):
"Define standard linear + softmax generation step."
def __init__(self, d_model, vocab):
super(Generator, self).__init__()
self.proj = nn.Linear(d_model, vocab)
def forward(self, x):
return log_softmax(self.proj(x), dim=-1)
~~~
__init__ 메서드에서 인코더, 디코더, 소스 임베딩(src_embed), 대상 임베딩(tgt_embed) 및 생성기가 초기화되고 EncoderDecoder 인스턴스의 각 속성에 할당됩니다.

forward 메소드는 소스 시퀀스(src), 대상 시퀀스(tgt), 소스 마스크(src_mask) 및 대상 마스크(tgt_mask)를 입력으로 사용합니다. 소스 임베딩 및 소스 마스크를 사용하여 인코더를 통과하여 소스 시퀀스를 처리합니다. 그 결과 인코딩된 표현은 소스 마스크, 대상 포함을 사용하여 포함된 대상 시퀀스 및 대상 마스크와 함께 디코더로 전달됩니다. decode 함수의 출력이 반환됩니다.

'encode' 메서드는 소스 시퀀스에 소스 임베딩을 적용한 다음 인코더에 전달합니다. 인코더의 출력을 반환합니다.

'decode' 방법은 인코딩된 표현('memory'), 소스 마스크, 대상 시퀀스 및 대상 마스크를 입력으로 사용합니다. 대상 포함을 사용하여 대상 시퀀스를 포함하고 다른 입력과 함께 디코더에 전달합니다. 디코더의 출력이 반환됩니다.

'Generator' 클래스는 별도로 정의되며 모델에서 최종 선형 변환 및 소프트맥스 생성 단계를 나타냅니다. 입력 x를 받고 선형 계층인 proj 모듈을 사용하여 선형 변환을 적용합니다. 출력은 대수 softmax 함수를 통해 전달되고 결과가 반환됩니다.

요약하면 'EncoderDecoder' 클래스는 전체 Transformer 기반 Encoder-Decoder 아키텍처를 캡슐화하고 'Generator' 클래스는 입력을 기반으로 출력 확률 생성을 처리합니다.





## Encoder
~~~
class Encoder(nn.Module):
"Core encoder is a stack of N layers"
def __init__(self, layer, N):
super(Encoder, self).__init__()
self.layers = clones(layer, N)
self.norm = LayerNorm(layer.size)
def forward(self, x, mask):
"Pass the input (and mask) through each layer in turn."
for layer in self.layers:
x = layer(x, mask)
return self.norm(x)

class EncoderLayer(nn.Module):
"Encoder is made up of self-attn and feed forward (defined below)"
def __init__(self, size, self_attn, feed_forward, dropout):
super(EncoderLayer, self).__init__()
self.self_attn = self_attn
self.feed_forward = feed_forward
self.sublayer = clones(SublayerConnection(size, dropout), 2)
self.size = size
def forward(self, x, mask):
"Follow Figure 1 (left) for connections."
x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
return self.sublayer[1](x, self.feed_forward)

class SublayerConnection(nn.Module):
"""
A residual connection followed by a layer norm.
Note for code simplicity the norm is first as opposed to last.
"""
def __init__(self, size, dropout):
super(SublayerConnection, self).__init__()
self.norm = LayerNorm(size)
self.dropout = nn.Dropout(dropout)
def forward(self, x, sublayer):
"Apply residual connection to any sublayer with the same size."
return x + self.dropout(sublayer(self.norm(x)))
~~~

Encoder 클래스는 N 레이어 스택인 코어 인코더를 나타냅니다. 레이어 모듈과 레이어 수 N을 입력으로 사용합니다. __init__ 메서드에서는 clones 함수를 사용하여 layer 모듈의 동일한 복사본을 생성하여 N 레이어를 초기화합니다. 이러한 레이어는 Encoder 인스턴스의 layers 속성에 저장됩니다. 또한 LayerNorm 모듈이 생성되어 norm 속성에 할당됩니다.


Encoder 클래스의 forward 메서드는 입력 x와 마스크를 입력으로 사용합니다. 스택의 각 레이어를 통해 입력과 마스크를 순차적으로 전달합니다. 각 레이어에 대해 입력 x는 마스크가 있는 레이어를 인수로 호출하여 업데이트됩니다. 모든 계층을 반복한 후 norm 모듈에서 수행한 계층 정규화를 사용하여 최종 출력 x를 정규화합니다. 정규화된 출력이 반환됩니다.


'EncoderLayer' 클래스는 인코더의 단일 레이어를 나타냅니다. 입력 x와 마스크를 입력으로 사용합니다. __init__ 메소드에서는 주어진 size, self_attn(셀프 어텐션 메커니즘), feed_forward(피드포워드 네트워크) 및 dropout으로 레이어를 초기화합니다. 레이어에는 SublayerConnection 클래스의 인스턴스인 sublayer 속성으로 표시되는 두 개의 하위 레이어가 있습니다. size 속성은 레이어 출력의 차원을 나타냅니다.


'EncoderLayer' 클래스의 'forward' 메소드는 트랜스포머 페이퍼(왼쪽)의 그림 1에 표시된 연결을 따릅니다. self_attn 모듈을 사용하여 입력 x에 self-attention 메커니즘을 적용합니다. 결과는 SublayerConnection 모듈을 사용하여 첫 번째 하위 계층을 통과합니다. 그런 다음 출력은 피드포워드 네트워크('feed_forward')를 나타내는 두 번째 하위 계층을 통과합니다. 레이어의 최종 출력이 반환됩니다.


SublayerConnection 클래스는 인코더 계층의 하위 계층을 나타냅니다. 잔여 연결을 적용한 다음 계층 정규화를 적용합니다. __init__ 메소드에서 주어진 크기 및 드롭아웃 비율로 레이어를 초기화합니다. 계층 정규화를 위한 norm 모듈과 dropout 모듈이 포함되어 있습니다. forward 메소드는 입력 x 및 sublayer 함수를 입력으로 사용합니다. 레이어 정규화를 x에 적용하고 하위 레이어를 통과한 다음 결과에 드롭아웃을 추가합니다. 출력은 'x'와 하위 계층 출력의 잔여 연결입니다.


요약하면 제공된 코드는 인코더 계층 스택을 나타내는 'Encoder' 클래스, 자체 주의 및 피드포워드 하위 계층이 있는 단일 인코더 계층을 나타내는 'EncoderLayer' 클래스를 포함하여 Transformer 모델의 인코더 구성 요소를 정의합니다. 계층 정규화를 사용한 잔여 연결을 나타내는 'SublayerConnection' 클래스. 이러한 구성 요소는 입력 시퀀스를 처리하고 Transformer 모델에서 컨텍스트 정보를 캡처하는 데 필수적입니다.

## Decoder
~~~
class Decoder(nn.Module):
"Generic N layer decoder with masking."
def __init__(self, layer, N):
super(Decoder, self).__init__()
self.layers = clones(layer, N)
self.norm = LayerNorm(layer.size)
def forward(self, x, memory, src_mask, tgt_mask):
for layer in self.layers:
x = layer(x, memory, src_mask, tgt_mask)
return self.norm(x)

class DecoderLayer(nn.Module):
"Decoder is made of self-attn, src-attn, and feed forward (defined below)"
def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
super(DecoderLayer, self).__init__()
self.size = size
self.self_attn = self_attn
self.src_attn = src_attn
self.feed_forward = feed_forward
self.sublayer = clones(SublayerConnection(size, dropout), 3)
def forward(self, x, memory, src_mask, tgt_mask):
"Follow Figure 1 (right) for connections."
m = memory
x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
return self.sublayer[2](x, self.feed_forward)

def subsequent_mask(size):
"Mask out subsequent positions."
attn_shape = (1, size, size)
subsequent_mask = torch.triu(torch.ones(attn_shape),
diagonal=1).type(torch.uint8)
return subsequent_mask == 0
~~~ 


제공된 코드는 Transformer 모델의 디코더와 관련된 구성 요소를 정의합니다.


Decoder 클래스는 N 레이어가 있는 일반 디코더를 나타냅니다. 레이어 모듈과 레이어 수 N을 입력으로 사용합니다. __init__ 메서드에서는 clones 함수를 사용하여 layer 모듈의 동일한 복사본을 생성하여 N 레이어를 초기화합니다. 이러한 레이어는 Decoder 인스턴스의 layers 속성에 저장됩니다. 또한 LayerNorm 모듈이 생성되어 norm 속성에 할당됩니다.


Decoder 클래스의 forward 메서드는 인코더의 메모리 텐서인 입력 x와 두 개의 마스크(src_mask 및 tgt_mask)를 사용합니다. 스택의 각 레이어를 반복하고 메모리, 소스 마스크 및 대상 마스크를 인수로 사용하여 레이어를 호출하여 입력 x를 업데이트합니다. 모든 계층을 반복한 후 norm 모듈에서 수행한 계층 정규화를 사용하여 최종 출력 x를 정규화합니다. 정규화된 출력이 반환됩니다.


DecoderLayer 클래스는 디코더의 단일 레이어를 나타냅니다. 인코더의 메모리 텐서인 입력 x와 src_mask 및 tgt_mask의 두 마스크를 입력으로 사용합니다. __init__ 메소드에서는 주어진 size, self_attn(디코더를 위한 자체 주의 메커니즘), src_attn(인코더 출력에 대한 소스 주의 메커니즘), feed_forward( 피드포워드 네트워크) 및 '드롭아웃'. 레이어에는 SublayerConnection 클래스의 인스턴스인 sublayer 속성으로 표시되는 3개의 하위 레이어가 있습니다. size 속성은 레이어 출력의 차원을 나타냅니다.


DecoderLayer 클래스의 forward 메서드는 Transformer 논문(오른쪽)의 그림 1에 표시된 연결을 따릅니다. self_attn 모듈을 사용하여 입력 x에 셀프 어텐션을 적용합니다. 결과는 SublayerConnection 모듈을 사용하여 첫 번째 하위 계층을 통과합니다. 그런 다음 'src_attn' 모듈을 사용하여 인코더 출력('memory' 텐서)을 처리하면서 첫 번째 하위 계층의 출력에 소스 주의를 적용합니다. 결과는 두 번째 하위 계층을 통과합니다. 마지막으로 출력은 피드포워드 네트워크('feed_forward')를 나타내는 세 번째 하위 계층을 통과합니다. 레이어의 최종 출력이 반환됩니다.


subsequent_mask 함수는 디코더의 자체 주의 메커니즘에서 후속 위치를 가리기 위해 마스크를 생성합니다. 어텐션 매트릭스의 '크기'를 입력으로 받아 위쪽 삼각형 부분(대각선 포함)이 0으로 채워지고 아래쪽 삼각형 부분이 1로 채워지는 마스크 텐서를 반환합니다. 이 마스크는 self-attention 계산 중에 입력의 위치가 후속 위치에 주의를 기울이는 것을 방지하는 데 유용합니다.


요약하면 제공된 코드는 디코더 레이어 스택을 나타내는 Decoder 클래스, 자체 주의, 소스 주의 및 피드포워드가 있는 단일 디코더 레이어를 나타내는 DecoderLayer 클래스를 포함하여 Transformer 모델의 디코더 구성 요소를 정의합니다. self-attention 메커니즘에서 후속 위치를 가리기 위해 마스크를 생성하는 'subsequent_mask' 기능. 이러한 구성 요소는 인코더 및 이전에 생성된 기호의 정보를 기반으로 출력 시퀀스를 디코딩하는 데 필수적입니다.

## Attention
~~~
def attention(query, key, value, mask=None, dropout=None):
"Compute 'Scaled Dot Product Attention'"
d_k = query.size(-1)
scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
if mask is not None:
scores = scores.masked_fill(mask == 0, -1e9)
p_attn = scores.softmax(dim=-1)
if dropout is not None:
p_attn = dropout(p_attn)
return torch.matmul(p_attn, value), p_attn

# 1) Do all the linear projections in batch from d_model => h x d_k
query, key, value = [
lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
for lin, x in zip(self.linears, (query, key, value))
]
# 2) Apply attention on all the projected vectors in batch.
x, self.attn = attention(
query, key, value, mask=mask, dropout=self.dropout
)
# 3) "Concat" using a view and apply a final linear.
x = (
x.transpose(1, 2)
.contiguous()
.view(nbatches, -1, self.h * self.d_k)
)

~~~

제공된 코드는 Transformer 모델에서 사용되는 주의 메커니즘을 구현합니다.


'attention' 함수는 쿼리, 키 및 값 텐서에 대해 스케일링된 내적 주의를 계산합니다. 입력 query의 모양은 [batch_size, h, seq_len_q, d_k]이고 key의 모양은 [batch_size, h, seq_len_k, d_k]입니다. value의 모양은 [batch_size, h, seq_len_v, d_v]입니다. 여기서 h는 어텐션 헤드, ​​seq_len_q, seq_len_k 및 seq_len의 수를 나타냅니다. _v는 각각 쿼리, 키 및 값의 시퀀스 길이를 나타내고 d_k 및 d_v는 키 및 값의 차원을 나타냅니다. 이 함수는 또한 특정 위치를 가리기 위한 선택적 mask 텐서와 주의 가중치에 드롭아웃을 적용하기 위한 dropout 모듈을 허용합니다.


먼저 쿼리 텐서에서 'd_k' 키의 크기를 얻습니다. 그런 다음 쿼리와 키 텐서의 내적을 'd_k'의 제곱근으로 나누어 어텐션 점수를 계산합니다. 마스크 텐서가 제공되면 큰 음수 값(이 경우 -1e9)을 할당하여 어텐션 점수의 특정 위치를 마스킹하는 데 사용됩니다. 마스킹된 점수는 주의 확률 'p_attn'을 얻기 위해 softmax 함수를 통과합니다. dropout 모듈이 제공되면 주의 확률에 적용됩니다.


이 함수는 torch.matmul(p_attn, value)로 표시된 주의 확률과 p_attn 자체의 주의 확률을 기반으로 값 텐서의 가중 합계를 반환합니다.


다음 코드 스니펫은 attention 기능의 사용법을 보여줍니다.


선형 프로젝션은 lin 선형 변환 모듈을 사용하여 쿼리, 키 및 값 텐서에 적용됩니다. 이것은 텐서를 재구성하고 선형 변환을 적용하여 일괄적으로 수행됩니다. 결과 텐서의 모양은 쿼리 및 키의 경우 [batch_size, h, seq_len, d_k]이고 값의 경우 [batch_size, h, seq_len, d_v]입니다.
어텐션 함수는 mask 및 dropout 인수와 함께 프로젝션된 쿼리, 키 및 값 텐서와 함께 호출됩니다. 결과 출력 'x'는 주의 확률을 기반으로 값 텐서의 가중 합계를 나타내고 'self.attn'에는 주의 확률이 포함됩니다.
마지막으로 출력 x는 형태가 변경되고 선형 변환되어 최종 출력을 얻습니다. 텐서는 형태가 [batch_size, seq_len, h * d_k]인 텐서를 얻기 위해 h 및 d_k 차원에서 전치, 재구성 및 연결됩니다.

요약하면 이 코드는 쿼리와 키 벡터 사이의 주의 가중치를 계산하고 해당 값 벡터에 적용하는 Transformer 모델에 사용되는 확장된 내적 주의 메커니즘을 구현합니다. 이 어텐션 메커니즘을 통해 모델은 인코딩 및 디코딩 프로세스 중에 입력 시퀀스의 다른 부분에서 관련 정보를 캡처할 수 있습니다.


## Position-wise Feed-Forward Networks
~~~
class PositionwiseFeedForward(nn.Module):
"Implements FFN equation."
def __init__(self, d_model, d_ff, dropout=0.1):
super(PositionwiseFeedForward, self).__init__()
self.w_1 = nn.Linear(d_model, d_ff)
self.w_2 = nn.Linear(d_ff, d_model)
self.dropout = nn.Dropout(dropout)
def forward(self, x):
return self.w_2(self.dropout(self.w_1(x).relu()
))
~~~

제공된 코드는 Transformer 모델에서 사용되는 위치별 피드포워드 네트워크를 구현합니다.


PositionwiseFeedForward 클래스는 ReLU 활성화 함수와 그 사이의 드롭아웃이 있는 두 개의 선형 레이어로 구성된 모듈을 정의합니다. 입력 x의 모양은 [batch_size, seq_len, d_model]이며 여기서 d_model은 입력 기능의 차원을 나타냅니다.


__init__ 메서드에서 두 개의 선형 레이어가 정의됩니다. 입력 크기가 d_model이고 출력 크기가 d_ff(중간 은닉 계층의 차원)인 self.w_1과 입력 크기가 self.w_2입니다. d_ff및 출력 크기d_model은 원래 입력 치수를 복원합니다. dropout` 매개변수는 탈락 확률을 지정합니다.


'forward' 방법에서는 입력 'x'가 첫 번째 선형 레이어 'self.w_1'을 통과한 다음 ReLU 활성화 함수가 이어집니다. 그런 다음 결과는 드롭아웃 레이어를 통과합니다. 마지막으로 출력은 두 번째 선형 레이어 'self.w_2'로 공급되어 위치별 피드포워드 네트워크의 최종 출력을 얻습니다.


Transformer 모델에서 위치별 피드포워드 네트워크의 목적은 각 위치에 개별적으로 동일하게 비선형 변환을 적용하는 것입니다. 이를 통해 모델은 시퀀스 내에서 복잡한 상호 작용 및 종속성을 캡처할 수 있습니다. ReLU 활성화 함수와 드롭아웃을 사용하면 각각 비선형성과 정규화를 도입하여 네트워크의 일반화와 견고성을 개선할 수 있습니다.


요약하면 이 코드는 ReLU 활성화 함수를 사용하여 선형 변환을 적용하고 입력 시퀀스의 각 위치에 독립적으로 드롭아웃하는 Transformer 모델에 사용되는 위치별 피드포워드 네트워크를 구현합니다.

## Positional Encoding
~~~
class PositionalEncoding(nn.Module):
"Implement the PE function."
def __init__(self, d_model, dropout, max_len=5000):
super(PositionalEncoding, self).__init__()
self.dropout = nn.Dropout(p=dropout)
# Compute the positional encodings once in log space.
pe = torch.zeros(max_len, d_model)
position = torch.arange(0, max_len).unsqueeze(1)
div_term = torch.exp(
torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
)
pe[:, 0::2] = torch.sin(position * div_term)
pe[:, 1::2] = torch.cos(position * div_term)
pe = pe.unsqueeze(0)
self.register_buffer("pe", pe)
def forward(self, x):
x = x + self.pe[:, : x.size(1)].requires_grad_(False)
return self.dropout(x)
~~~

제공된 코드는 Transformer 모델에서 사용되는 위치 인코딩을 구현합니다.


PositionalEncoding 클래스는 입력 시퀀스에 위치 인코딩을 추가하는 모듈을 정의합니다. 위치 인코딩은 시퀀스에서 토큰의 상대적 위치를 캡처하여 모델이 토큰의 순서를 이해할 수 있도록 합니다. 이러한 위치 인코딩은 모델의 입력 임베딩에 추가됩니다.


__init__ 메소드에서 위치 인코딩이 계산되어 버퍼 pe에 저장됩니다. 위치 인코딩은 [1, max_len, d_model] 형태를 가지며, 여기서 max_len은 최대 시퀀스 길이를 나타내고 d_model은 모델의 차원을 나타냅니다.


위치 인코딩은 주파수가 다른 사인 및 코사인 함수를 사용하여 계산됩니다. 시퀀스의 각 위치에 대해 위치 인코딩 벡터는 사인 및 코사인 함수에 서로 다른 주파수를 적용하여 계산됩니다. 빈도는 'div_term' 변수에 의해 결정되며 위치 및 치수를 기반으로 지수 값을 사용하여 계산됩니다. 짝수 및 홀수 차원에 대한 위치 인코딩은 'pe' 텐서의 해당 열에 할당됩니다.


'forward' 방법에서는 위치 인코딩이 입력 텐서 'x'에 추가되지만 입력 시퀀스의 길이까지만 추가됩니다. 이는 입력 및 위치 인코딩의 크기를 일치시키기 위해 수행됩니다. 그런 다음 결과 텐서는 정규화를 위해 드롭아웃 레이어를 통과합니다.


위치 인코딩의 목적은 시퀀스의 토큰 순서에 대한 정보를 모델에 제공하는 것입니다. 입력 임베딩에 위치 인코딩을 추가함으로써 모델은 위치에 따라 토큰을 구별할 수 있습니다. 이를 통해 Transformer 모델은 반복적 또는 컨벌루션 작업에 의존하지 않고 순차 종속성을 캡처하고 토큰의 상대적 위치를 이해할 수 있습니다.


요약하면 이 코드는 사인 및 코사인 함수를 기반으로 입력 시퀀스에 위치 인코딩을 추가하는 Transformer 모델에서 사용되는 위치 인코딩을 구현합니다. 이러한 위치 인코딩은 토큰 순서에 대한 정보를 제공하고 모델이 순차적 종속성을 캡처하는 데 도움이 됩니다.

## Full Mode
~~~
def make_model(
src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1
):
"Helper: Construct a model from hyperparameters."
c = copy.deepcopy
attn = MultiHeadedAttention(h, d_model)
ff = PositionwiseFeedForward(d_model, d_ff, dropout)
position = PositionalEncoding(d_model, dropout)
model = EncoderDecoder(
Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),
nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
Generator(d_model, tgt_vocab),
)
# This was important from their code.
# Initialize parameters with Glorot / fan_avg.
for p in model.parameters():
if p.dim() > 1:
nn.init.xavier_uniform_(p)
return model
~~~

제공된 코드는 주어진 하이퍼파라미터를 기반으로 Transformer 모델을 구성하는 make_model 함수를 정의합니다.


make_model 함수는 여러 인수를 사용합니다.


src_vocab: 소스 어휘의 크기(고유 토큰 수).
tgt_vocab: 대상 어휘의 크기.
N: 모델의 인코더 및 디코더 레이어 수.
d_model: 모델의 차원.
d_ff: 모델에서 피드포워드 네트워크의 차원.
h: 어텐션 헤드의 수.
dropout: 모델에서 사용된 탈락 확률.

함수 내에서 코드는 주의 메커니즘, 피드포워드 네트워크, 위치 인코딩 및 모델에 필요한 기타 구성 요소를 초기화합니다.


attn은 MultiHeadedAttention 클래스의 인스턴스로, 모델에서 사용되는 다중 헤드 어텐션 메커니즘을 나타냅니다.
ff는 PositionwiseFeedForward 클래스의 인스턴스로, 모델에서 사용되는 위치별 피드포워드 네트워크를 나타냅니다.
position은 입력 시퀀스에 위치 인코딩을 제공하는 PositionalEncoding 클래스의 인스턴스입니다.

Transformer 모델의 전체 Encoder-Decoder 아키텍처를 나타내는 'EncoderDecoder' 클래스가 인스턴스화됩니다. 인코더, 디코더, 소스 및 대상 임베딩, 생성기로 구성됩니다.


인코더는 self-attention 및 feed-forward 레이어로 구성된 'EncoderLayer' 인스턴스를 사용하여 생성됩니다. 레이어 수는 'N' 인수로 결정됩니다.
디코더는 self-attention, source Attention, Feed-forward layer로 구성된 DecoderLayer 인스턴스를 사용하여 생성됩니다. 레이어 수 역시 'N'으로 결정됩니다.
소스 및 대상 임베딩은 Embeddings 클래스를 사용하여 생성되고 position에서 제공하는 위치 인코딩과 결합됩니다.
생성기는 출력 확률을 생성하기 위해 선형 변환 및 소프트맥스 활성화를 적용하는 Generator 클래스의 인스턴스입니다.

마지막으로 모델의 매개변수는 Glorot(Xavier라고도 함) 초기화 방법을 사용하여 초기화되어 훈련 중에 더 나은 기울기 흐름에 도움이 됩니다.


make_model 함수는 구성된 Transformer 모델을 반환합니다.


요약하면, 'make_model' 함수는 레이어 수, 모델 차원, 피드포워드 차원, 어텐션 헤드 수 및 드롭아웃 비율을 포함하여 지정된 하이퍼파라미터로 Transformer 모델을 생성하는 편리한 방법을 제공합니다.

## Inference
~~~
memory = test_model.encode(src, src_mask)
ys = torch.zeros(1, 1).type_as(src)
for i in range(9):
print(ys)
out = test_model.decode(
memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)
)
prob = test_model.generator(out[:, -1])
_, next_word = torch.max(prob, dim=1)
next_word = next_word.data[0]
ys = torch.cat(
[ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1
)
print("Example Untrained Model Prediction:", ys)
~~~

제공된 코드는 훈련된 Transformer 모델을 사용하여 추론을 수행하는 방법의 예를 보여줍니다.


추론 프로세스가 작동하는 방식은 다음과 같습니다.


memory = test_model.encode(src, src_mask): 소스 시퀀스 src는 test_model의 인코더를 통과하여 인코딩됩니다. src_mask는 소스 시퀀스의 패딩 토큰을 마스킹하는 데 사용됩니다.
ys = torch.zeros(1, 1).type_as(src): 초기 대상 시퀀스 ys는 단일 토큰으로 생성됩니다. 이것은 출력 시퀀스를 생성하기 위한 시작점 역할을 합니다.
for i in range(9):: 다음 단계는 최대 9회까지 반복됩니다(필요에 따라 이 값을 수정할 수 있음).
print(ys): 대상 시퀀스 ys의 현재 상태가 인쇄됩니다.
out = test_model.decode(memory, src_mask, ys, 후속_마스크(ys.size(1)).type_as(src.data)): test_model은 대상 시퀀스 ys를 통과시켜 디코딩합니다. 디코더. memory에는 인코딩된 소스 시퀀스가 ​​포함되며 src_mask는 소스 시퀀스의 모든 패딩 토큰을 마스킹하는 데 사용됩니다. 'subsequent_mask'는 모델이 출력 시퀀스의 미래 위치에 주의를 기울이는 것을 방지하기 위해 적용됩니다.
prob = test_model.generator(out[:, -1]): 디코더의 출력, 특히 출력 시퀀스의 마지막 토큰은 test_model의 생성기를 통해 전달됩니다. 이는 대상 어휘에 대한 확률 분포를 생성합니다.
_, next_word = torch.max(prob, dim=1): 목표 시퀀스에서 확률이 가장 높은 단어가 다음 단어로 선택됩니다.
next_word = next_word.data[0]: 선택한 단어의 색인을 추출합니다.
ys = torch.cat([ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1): 선택한 단어가 대상 시퀀스에 추가됩니다  ys는 torch.cat()을 사용하여 하나의 토큰으로 시퀀스를 확장합니다.
print("Example Untrained Model Prediction:", ys): 대상 시퀀스 ys의 현재 상태가 인쇄되어 출력 시퀀스에 대한 모델의 예측을 보여줍니다.

이 프로세스는 이전에 생성된 토큰을 다음 토큰 예측을 위한 컨텍스트로 사용하여 토큰별로 출력 시퀀스 토큰을 생성합니다. 최대 반복 횟수에 도달하거나 작업에 특정한 종료 조건이 충족될 때까지 계속됩니다.


제공된 코드에서 모델은 테스트 목적으로 사용되고 있음을 나타내는 'test_model'이라고 합니다. 또한 이 코드 스니펫은 데모용이므로 주석에 표시된 대로 모델이 훈련되지 않은 것으로 가정합니다.