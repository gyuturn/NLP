# Neural Machine Translation(2)

~~~
# For tips on running notebooks in Google Colab, see
# https://pytorch.org/tutorials/beginner/colab
%matplotlib inline




# Translation with a Sequence to Sequence Network and Attention
**Author**: [Sean Robertson](https://github.com/spro/practical-pytorch)

This is the third and final tutorial on doing "NLP From Scratch", where we
write our own classes and functions to preprocess the data to do our NLP
modeling tasks. We hope after you complete this tutorial that you'll proceed to
learn how `torchtext` can handle much of this preprocessing for you in the
three tutorials immediately following this one.

In this project we will be teaching a neural network to translate from
French to English.

::

    [KEY: > input, = target, < output]

    > il est en train de peindre un tableau .
    = he is painting a picture .
    < he is painting a picture .

    > pourquoi ne pas essayer ce vin delicieux ?
    = why not try that delicious wine ?
    < why not try that delicious wine ?

    > elle n est pas poete mais romanciere .
    = she is not a poet but a novelist .
    < she not not a poet but a novelist .

    > vous etes trop maigre .
    = you re too skinny .
    < you re all alone .

... to varying degrees of success.

This is made possible by the simple but powerful idea of the [sequence
to sequence network](https://arxiv.org/abs/1409.3215)_, in which two
recurrent neural networks work together to transform one sequence to
another. An encoder network condenses an input sequence into a vector,
and a decoder network unfolds that vector into a new sequence.


![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)

To improve upon this model we'll use an [attention
mechanism](https://arxiv.org/abs/1409.0473)_, which lets the decoder
learn to focus over a specific range of the input sequence.

**Recommended Reading:**

I assume you have at least installed PyTorch, know Python, and
understand Tensors:

-  https://pytorch.org/ For installation instructions
-  [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz) to get started with PyTorch in general
-  [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples) for a wide and deep overview
-  [PyTorch for Former Torch Users](https://pytorch.org/tutorials/beginner/former_torchies_tutorial) if you are former Lua Torch user


It would also be useful to know about Sequence to Sequence networks and
how they work:

-  [Learning Phrase Representations using RNN Encoder-Decoder for
   Statistical Machine Translation](https://arxiv.org/abs/1406.1078)_
-  [Sequence to Sequence Learning with Neural
   Networks](https://arxiv.org/abs/1409.3215)_
-  [Neural Machine Translation by Jointly Learning to Align and
   Translate](https://arxiv.org/abs/1409.0473)_
-  [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)_

**Requirements**


from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

## Loading data files

The data for this project is a set of many thousands of English to
French translation pairs.

[This question on Open Data Stack
Exchange](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)_
pointed me to the open translation site https://tatoeba.org/ which has
downloads available at https://tatoeba.org/eng/downloads - and better
yet, someone did the extra work of splitting language pairs into
individual text files here: https://www.manythings.org/anki/

The English to French pairs are too big to include in the repository, so
download to ``data/eng-fra.txt`` before continuing. The file is a tab
separated list of translation pairs:

::

    I am cold.    J'ai froid.

.. Note::
   Download the data from
   [here](https://download.pytorch.org/tutorial/data.zip)
   and extract it to the current directory.



Similar to the character encoding used in the character-level RNN
tutorials, we will be representing each word in a language as a one-hot
vector, or giant vector of zeros except for a single one (at the index
of the word). Compared to the dozens of characters that might exist in a
language, there are many many more words, so the encoding vector is much
larger. We will however cheat a bit and trim the data to only use a few
thousand words per language.

![](https://pytorch.org/tutorials/_static/img/seq-seq-images/word-encoding.png)





We'll need a unique index per word to use as the inputs and targets of
the networks later. To keep track of all this we will use a helper class
called ``Lang`` which has word → index (``word2index``) and index → word
(``index2word``) dictionaries, as well as a count of each word
``word2count`` which will be used to replace rare words later.




!wget https://download.pytorch.org/tutorial/data.zip
!unzip data.zip

SOS_token = 0
EOS_token = 1


class Lang:
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = {}
        self.index2word = {0: "SOS", 1: "EOS"}
        self.n_words = 2  # Count SOS and EOS

    def addSentence(self, sentence):
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

The files are all in Unicode, to simplify we will turn Unicode
characters to ASCII, make everything lowercase, and trim most
punctuation.




# Turn a Unicode string to plain ASCII, thanks to
# https://stackoverflow.com/a/518232/2809427
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
    )

# Lowercase, trim, and remove non-letter characters


def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

To read the data file we will split the file into lines, and then split
lines into pairs. The files are all English → Other Language, so if we
want to translate from Other Language → English I added the ``reverse``
flag to reverse the pairs.




def readLangs(lang1, lang2, reverse=False):
    print("Reading lines...")

    # Read the file and split into lines
    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\
        read().strip().split('\n')

    # Split every line into pairs and normalize
    pairs = [[normalizeString(s) for s in l.split('\t')] for l in lines]

    # Reverse pairs, make Lang instances
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(lang2)
        output_lang = Lang(lang1)
    else:
        input_lang = Lang(lang1)
        output_lang = Lang(lang2)

    return input_lang, output_lang, pairs

Since there are a *lot* of example sentences and we want to train
something quickly, we'll trim the data set to only relatively short and
simple sentences. Here the maximum length is 10 words (that includes
ending punctuation) and we're filtering to sentences that translate to
the form "I am" or "He is" etc. (accounting for apostrophes replaced
earlier).




MAX_LENGTH = 10

eng_prefixes = (
    "i am ", "i m ",
    "he is", "he s ",
    "she is", "she s ",
    "you are", "you re ",
    "we are", "we re ",
    "they are", "they re "
)


def filterPair(p):
    return len(p[0].split(' ')) < MAX_LENGTH and \
        len(p[1].split(' ')) < MAX_LENGTH and \
        p[1].startswith(eng_prefixes)


def filterPairs(pairs):
    return [pair for pair in pairs if filterPair(pair)]

The full process for preparing the data is:

-  Read text file and split into lines, split lines into pairs
-  Normalize text, filter by length and content
-  Make word lists from sentences in pairs




def prepareData(lang1, lang2, reverse=False):
    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)
    print("Read %s sentence pairs" % len(pairs))
    pairs = filterPairs(pairs)
    print("Trimmed to %s sentence pairs" % len(pairs))
    print("Counting words...")
    for pair in pairs:
        input_lang.addSentence(pair[0])
        output_lang.addSentence(pair[1])
    print("Counted words:")
    print(input_lang.name, input_lang.n_words)
    print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs


input_lang, output_lang, pairs = prepareData('eng', 'fra', True)
print(random.choice(pairs))

## The Seq2Seq Model

A Recurrent Neural Network, or RNN, is a network that operates on a
sequence and uses its own output as input for subsequent steps.

A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215)_, or
seq2seq network, or [Encoder Decoder
network](https://arxiv.org/pdf/1406.1078v3.pdf)_, is a model
consisting of two RNNs called the encoder and decoder. The encoder reads
an input sequence and outputs a single vector, and the decoder reads
that vector to produce an output sequence.

![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)

Unlike sequence prediction with a single RNN, where every input
corresponds to an output, the seq2seq model frees us from sequence
length and order, which makes it ideal for translation between two
languages.

Consider the sentence ``Je ne suis pas le chat noir`` → ``I am not the
black cat``. Most of the words in the input sentence have a direct
translation in the output sentence, but are in slightly different
orders, e.g. ``chat noir`` and ``black cat``. Because of the ``ne/pas``
construction there is also one more word in the input sentence. It would
be difficult to produce a correct translation directly from the sequence
of input words.

With a seq2seq model the encoder creates a single vector which, in the
ideal case, encodes the "meaning" of the input sequence into a single
vector — a single point in some N dimensional space of sentences.




### The Encoder

The encoder of a seq2seq network is a RNN that outputs some value for
every word from the input sentence. For every input word the encoder
outputs a vector and a hidden state, and uses the hidden state for the
next input word.

![](https://pytorch.org/tutorials/_static/img/seq-seq-images/encoder-network.png)




class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output = embedded
        output, hidden = self.gru(output, hidden)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

### The Decoder

The decoder is another RNN that takes the encoder output vector(s) and
outputs a sequence of words to create the translation.




#### Simple Decoder

In the simplest seq2seq decoder we use only last output of the encoder.
This last output is sometimes called the *context vector* as it encodes
context from the entire sequence. This context vector is used as the
initial hidden state of the decoder.

At every step of decoding, the decoder is given an input token and
hidden state. The initial input token is the start-of-string ``<SOS>``
token, and the first hidden state is the context vector (the encoder's
last hidden state).

![](https://pytorch.org/tutorials/_static/img/seq-seq-images/decoder-network.png)




class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output, hidden = self.gru(output, hidden)
        output = self.softmax(self.out(output[0]))
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

I encourage you to train and observe the results of this model, but to
save space we'll be going straight for the gold and introducing the
Attention Mechanism.




#### Attention Decoder

If only the context vector is passed between the encoder and decoder,
that single vector carries the burden of encoding the entire sentence.

Attention allows the decoder network to "focus" on a different part of
the encoder's outputs for every step of the decoder's own outputs. First
we calculate a set of *attention weights*. These will be multiplied by
the encoder output vectors to create a weighted combination. The result
(called ``attn_applied`` in the code) should contain information about
that specific part of the input sequence, and thus help the decoder
choose the right output words.

![](https://i.imgur.com/1152PYf.png)

Calculating the attention weights is done with another feed-forward
layer ``attn``, using the decoder's input and hidden state as inputs.
Because there are sentences of all sizes in the training data, to
actually create and train this layer we have to choose a maximum
sentence length (input length, for encoder outputs) that it can apply
to. Sentences of the maximum length will use all the attention weights,
while shorter sentences will only use the first few.

![](https://pytorch.org/tutorials/_static/img/seq-seq-images/attention-decoder-network.png)





class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding = nn.Embedding(self.output_size, self.hidden_size)
        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        self.gru = nn.GRU(self.hidden_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        embedded = self.dropout(embedded)

        attn_weights = F.softmax(
            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)

        output = F.relu(output)
        output, hidden = self.gru(output, hidden)

        output = F.log_softmax(self.out(output[0]), dim=1)
        return output, hidden, attn_weights

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

There are other forms of attention that work around the length
  limitation by using a relative position approach. Read about "local
  attention" in [Effective Approaches to Attention-based Neural Machine
  Translation](https://arxiv.org/abs/1508.04025).

## Training

### Preparing Training Data

To train, for each pair we will need an input tensor (indexes of the
words in the input sentence) and target tensor (indexes of the words in
the target sentence). While creating these vectors we will append the
EOS token to both sequences.




def indexesFromSentence(lang, sentence):
    return [lang.word2index[word] for word in sentence.split(' ')]


def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)


def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)

### Training the Model

To train we run the input sentence through the encoder, and keep track
of every output and the latest hidden state. Then the decoder is given
the ``<SOS>`` token as its first input, and the last hidden state of the
encoder as its first hidden state.

"Teacher forcing" is the concept of using the real target outputs as
each next input, instead of using the decoder's guess as the next input.
Using teacher forcing causes it to converge faster but [when the trained
network is exploited, it may exhibit
instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).

You can observe outputs of teacher-forced networks that read with
coherent grammar but wander far from the correct translation -
intuitively it has learned to represent the output grammar and can "pick
up" the meaning once the teacher tells it the first few words, but it
has not properly learned how to create the sentence from the translation
in the first place.

Because of the freedom PyTorch's autograd gives us, we can randomly
choose to use teacher forcing or not with a simple if statement. Turn
``teacher_forcing_ratio`` up to use more of it.




teacher_forcing_ratio = 0.5


def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,
          decoder_optimizer, criterion, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden()

    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(
            input_tensor[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS_token]], device=device)

    decoder_hidden = encoder_hidden

    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False

    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # Teacher forcing

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS_token:
                break

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return loss.item() / target_length

This is a helper function to print time elapsed and estimated time
remaining given the current time and progress %.




import time
import math


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

The whole training process looks like this:

-  Start a timer
-  Initialize optimizers and criterion
-  Create set of training pairs
-  Start empty losses array for plotting

Then we call ``train`` many times and occasionally print the progress (%
of examples, time so far, estimated time) and average loss.




def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    training_pairs = [tensorsFromPair(random.choice(pairs))
                      for i in range(n_iters)]
    criterion = nn.NLLLoss()

    for iter in range(1, n_iters + 1):
        training_pair = training_pairs[iter - 1]
        input_tensor = training_pair[0]
        target_tensor = training_pair[1]

        loss = train(input_tensor, target_tensor, encoder,
                     decoder, encoder_optimizer, decoder_optimizer, criterion)
        print_loss_total += loss
        plot_loss_total += loss

        if iter % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),
                                         iter, iter / n_iters * 100, print_loss_avg))

        if iter % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0

    showPlot(plot_losses)

### Plotting results

Plotting is done with matplotlib, using the array of loss values
``plot_losses`` saved while training.




import matplotlib.pyplot as plt
plt.switch_backend('agg')
import matplotlib.ticker as ticker
import numpy as np


def showPlot(points):
    plt.figure()
    fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)

## Evaluation

Evaluation is mostly the same as training, but there are no targets so
we simply feed the decoder's predictions back to itself for each step.
Every time it predicts a word we add it to the output string, and if it
predicts the EOS token we stop there. We also store the decoder's
attention outputs for display later.




def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):
    with torch.no_grad():
        input_tensor = tensorFromSentence(input_lang, sentence)
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size,
                                      device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden = encoder(input_tensor[ei],
                                                     encoder_hidden)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS

        decoder_hidden = encoder_hidden

        decoded_words = []
        decoder_attentions = torch.zeros(max_length, max_length)

        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            decoder_attentions[di] = decoder_attention.data
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == EOS_token:
                decoded_words.append('<EOS>')
                break
            else:
                decoded_words.append(output_lang.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return decoded_words, decoder_attentions[:di + 1]

We can evaluate random sentences from the training set and print out the
input, target, and output to make some subjective quality judgements:




def evaluateRandomly(encoder, decoder, n=10):
    for i in range(n):
        pair = random.choice(pairs)
        print('>', pair[0])
        print('=', pair[1])
        output_words, attentions = evaluate(encoder, decoder, pair[0])
        output_sentence = ' '.join(output_words)
        print('<', output_sentence)
        print('')

## Training and Evaluating

With all these helper functions in place (it looks like extra work, but
it makes it easier to run multiple experiments) we can actually
initialize a network and start training.

Remember that the input sentences were heavily filtered. For this small
dataset we can use relatively small networks of 256 hidden nodes and a
single GRU layer. After about 40 minutes on a MacBook CPU we'll get some
reasonable results.

.. Note::
   If you run this notebook you can train, interrupt the kernel,
   evaluate, and continue training later. Comment out the lines where the
   encoder and decoder are initialized and run ``trainIters`` again.




hidden_size = 256
encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)
attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)

trainIters(encoder1, attn_decoder1, 75000, print_every=5000)

evaluateRandomly(encoder1, attn_decoder1)

### Visualizing Attention

A useful property of the attention mechanism is its highly interpretable
outputs. Because it is used to weight specific encoder outputs of the
input sequence, we can imagine looking where the network is focused most
at each time step.

You could simply run ``plt.matshow(attentions)`` to see attention output
displayed as a matrix, with the columns being input steps and rows being
output steps:




output_words, attentions = evaluate(
    encoder1, attn_decoder1, "je suis trop froid .")
plt.matshow(attentions.numpy())

For a better viewing experience we will do the extra work of adding axes
and labels:




def showAttention(input_sentence, output_words, attentions):
    # Set up figure with colorbar
    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(attentions.numpy(), cmap='bone')
    fig.colorbar(cax)

    # Set up axes
    ax.set_xticklabels([''] + input_sentence.split(' ') +
                       ['<EOS>'], rotation=90)
    ax.set_yticklabels([''] + output_words)

    # Show label at every tick
    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

    plt.show()


def evaluateAndShowAttention(input_sentence):
    output_words, attentions = evaluate(
        encoder1, attn_decoder1, input_sentence)
    print('input =', input_sentence)
    print('output =', ' '.join(output_words))
    showAttention(input_sentence, output_words, attentions)


evaluateAndShowAttention("elle a cinq ans de moins que moi .")

evaluateAndShowAttention("elle est trop petit .")

evaluateAndShowAttention("je ne crains pas de mourir .")

evaluateAndShowAttention("c est un jeune directeur plein de talent .")
~~~

1. 시퀀스-시퀀스 모델: seq2seq 모델은 인코더와 디코더라는 두 개의 순환 신경망(RNN)으로 구성된 일종의 신경망 아키텍처입니다. 인코더는 입력 시퀀스를 처리하고 고정 길이 벡터로 인코딩하여 시퀀스의 의미를 캡처합니다. 디코더는 이 벡터를 가져와 이를 기반으로 출력 시퀀스를 생성합니다.
2. 인코더-디코더 아키텍처: 인코더 네트워크는 입력 시퀀스(프랑스어 문장)를 단어 단위로 읽고 숨겨진 상태 시퀀스를 생성합니다. 이전 단어에서 학습된 정보를 나타내는 숨겨진 상태를 유지하기 위해 순환 계층(GRU)을 사용합니다. 인코더의 최종 숨겨진 상태는 디코더로 전달됩니다.
3. 주의 메커니즘: 주의 메커니즘을 통해 디코더는 출력 시퀀스를 생성하는 동안 입력 시퀀스의 다른 부분에 집중할 수 있습니다. 인코더의 마지막 은닉 상태에만 의존하는 대신 디코더는 각 단계에서 모든 인코더 은닉 상태의 가중 합을 사용합니다. 이러한 가중치는 동적으로 학습되며 현재 디코딩 단계에 대한 각 숨겨진 상태의 중요성을 나타냅니다.
4. Word Embeddings: 입력 및 출력 언어의 단어는 원-핫 벡터로 표현됩니다. 그러나 단어 간의 의미론적 유사성을 포착하기 위해 모델은 단어 임베딩을 사용합니다. 단어 임베딩은 단어를 연속적이고 조밀한 벡터 표현에 매핑하여 모델이 단어 간의 관계를 학습할 수 있도록 합니다.
5. 언어 데이터 준비: 코드는 언어 데이터를 준비하기 위해 여러 단계를 수행합니다. 파일에서 번역 쌍을 읽고 문장을 정규화 및 정리하고 길이와 내용을 기준으로 필터링합니다. 또한 단어를 고유 인덱스에 매핑하기 위해 어휘 사전(word2index 및 index2word)을 생성합니다.
6. 훈련: 코드에는 준비된 데이터 쌍을 반복하는 훈련 루프가 포함되어 있습니다. 입력 문장을 인코더에 공급하고 주의를 기울여 디코더를 사용하여 출력 문장을 생성하고 예측 출력과 대상 문장 사이의 손실을 계산합니다. 그런 다음 손실이 역전파되어 모델의 매개변수를 업데이트합니다.
7. 모델 평가: 학습 후 코드는 새로운 입력 문장에서 모델의 성능을 평가하는 기능을 제공합니다. 인코더를 통해 입력을 공급하고 디코더를 사용하여 출력 시퀀스를 생성하여 번역을 수행합니다.

## 코드에 대한 추가 설명
코드에서 인코더와 디코더 구성 요소는 RNN, 특히 GRU(Gated Recurrent Unit)를 활용하여 각각 입력 및 출력 시퀀스를 처리합니다. GRU는 기울기 소실 문제를 해결하는 데 도움이 되고 시간에 따른 더 나은 기울기 흐름을 허용하는 기본 RNN의 변형입니다.


인코더 RNN은 입력 시퀀스를 단어 단위로 처리하고 이전 단어에서 학습한 정보를 요약하는 숨겨진 상태를 유지합니다. 각 시간 단계에서 숨겨진 상태는 현재 입력 단어와 이전 숨겨진 상태를 기반으로 업데이트됩니다. RNN의 반복적 특성으로 인해 입력 문장에서 순차적 종속성을 캡처할 수 있습니다.


다음은 코드에서 RNN 개념을 설명하는 예입니다.


입력 문장(영어): "I'm going to the beach."
토큰화된 입력: ['I', 'going', 'to', 'the', 'beach', '.']


초기화: 인코더 RNN의 초기 숨겨진 상태는 0으로 설정되거나 무작위로 초기화됩니다.
시간 단계 1: RNN은 초기 숨겨진 상태와 함께 첫 번째 입력 단어 'Je'를 처리합니다. 입력을 기반으로 숨겨진 상태를 업데이트하고 출력을 생성합니다.
시간 단계 2: RNN은 두 번째 입력 단어 'vais'와 이전 ​​단계에서 업데이트된 숨겨진 상태를 입력으로 사용합니다. 숨겨진 상태를 다시 업데이트하고 다른 출력을 생성합니다.
시간 단계 3-5: RNN은 입력 시퀀스의 나머지 단어를 계속 처리하여 숨겨진 상태를 업데이트하고 각 시간 단계에서 출력을 생성합니다.
최종 은닉 상태: 전체 입력 시퀀스를 처리한 후 RNN의 최종 은닉 상태는 이전의 모든 단어에서 학습된 정보를 캡처하여 입력 문장의 요약을 나타냅니다.

디코더 RNN은 유사한 프로세스를 따르지만 인코더의 최종 숨겨진 상태를 초기 숨겨진 상태로 사용합니다. 현재 입력 단어와 이전 시간 단계의 숨겨진 상태를 고려하여 단어별로 출력 시퀀스(영어 번역)를 생성합니다.


전반적으로 RNN의 반복적 특성으로 인해 순차적 종속성을 처리하고 과거 시간 단계의 정보를 인코딩/디코딩할 수 있으므로 모델이 기계 번역에 필요한 컨텍스트 정보를 캡처할 수 있습니다.

# Transformer

Transformer는 Vaswani 등의 논문 "Attention Is All You Need"에 소개된 딥 러닝 모델 아키텍처입니다. 2017년에 상당한 주목을 받았으며 자연어 처리 및 기계 번역 작업에서 가장 영향력 있는 아키텍처 중 하나가 되었습니다.


Transformer 아키텍처는 어텐션 메커니즘만을 기반으로 하며 시퀀스 모델링 작업에서 일반적으로 사용되는 순환 신경망(RNN) 또는 컨볼루션 신경망(CNN)에 의존하지 않습니다. 시퀀스의 단어 또는 토큰 간의 종속성 및 관계를 효율적으로 캡처하도록 설계되었습니다.


### 변압기의 주요 구성 요소:
    인코더: 입력 시퀀스는 동일한 레이어의 스택으로 구성된 인코더에 의해 처리됩니다. 인코더의 각 계층에는 셀프 어텐션 메커니즘과 피드포워드 신경망이라는 두 개의 하위 계층이 있습니다. self-attention 메커니즘을 통해 모델은 입력 시퀀스에서 서로 다른 단어 간의 관계를 캡처할 수 있으며 피드포워드 네트워크는 복잡한 패턴과 변환을 캡처하는 데 도움이 됩니다.
    디코더: 디코더도 동일한 레이어 스택으로 구성됩니다. 디코더의 각 계층에는 셀프 어텐션, 인코더-디코더 어텐션 및 피드포워드 네트워크의 세 가지 하위 계층이 있습니다. 디코더의 셀프 어텐션 메커니즘은 디코더가 디코딩 순서에서 다른 위치에 주의를 기울일 수 있도록 하여 올바른 순서로 단어를 생성하도록 돕습니다. 인코더-디코더 어텐션은 디코더가 인코더 출력의 정보를 활용하여 변환 또는 생성 프로세스를 용이하게 합니다.
    어텐션 메커니즘: Transformer의 어텐션 메커니즘을 통해 모델은 출력을 생성하는 동안 입력 시퀀스의 다른 부분에 집중할 수 있습니다. 현재 출력 위치에 대한 해당 위치의 중요성 또는 관련성을 결정하는 각 입력 위치에 대한 주의 가중치를 계산합니다. 어텐션 메커니즘을 사용하면 모델이 장거리 종속성을 효과적으로 캡처하고 입력 시퀀스 내의 컨텍스트를 이해할 수 있습니다.

### 변압기의 이점:
    병렬성: Transformer 아키텍처는 어텐션 메커니즘을 통해 각 단어 또는 토큰을 독립적으로 처리할 수 있으므로 효율적인 병렬 계산이 가능합니다. 이는 RNN과 같은 순차 모델에 비해 훈련 및 추론 시간을 단축합니다.
    장거리 종속성: Transformer의 self-attention 메커니즘은 모델이 입력 시퀀스에서 멀리 떨어져 있는 단어 간의 종속성을 캡처할 수 있도록 합니다. 이는 기계 번역과 같은 장기적인 맥락 이해와 관련된 작업에 특히 유용합니다.
    확장성: 트랜스포머는 자르거나 패딩할 필요 없이 가변 길이의 입력 시퀀스를 처리할 수 있습니다. 이러한 유연성 덕분에 다양한 작업에 적합하고 단어 또는 토큰 간의 관계를 효과적으로 모델링할 수 있습니다.

    Transformer 아키텍처는 기계 번역, 텍스트 요약, 질문 응답 및 언어 생성을 비롯한 다양한 자연어 처리 작업에 널리 채택되었습니다. 종속성을 캡처하고, 계산을 병렬화하고, 장거리 관계를 모델링하는 기능으로 인해 시퀀스 모델링 작업을 위한 강력하고 대중적인 선택이 되었습니다.

## Attention Mecahnism
    주의 메커니즘은 모델이 출력을 생성할 때 입력 시퀀스의 다른 부분에 선택적으로 집중할 수 있도록 하는 Transformer 아키텍처의 구성 요소입니다. 입력 시퀀스의 각 위치에 대한 주의 가중치를 계산하여 현재 출력에 대한 해당 위치의 관련성 또는 중요도를 나타냅니다.


    어텐션 메커니즘을 이해하기 위해 "I love dogs"라는 문장을 영어에서 프랑스어로 번역하려는 기계 번역 작업의 예를 들어 보겠습니다. 어텐션 메커니즘은 모델이 입력 문장의 어떤 단어가 번역된 문장에서 해당 단어를 생성하는 데 가장 관련이 있는지 결정하는 데 도움이 됩니다.


    번역 프로세스 중에 모델은 여러 디코딩 단계를 거쳐 한 번에 하나의 단어를 생성합니다. 각 디코딩 단계에서 어텐션 메커니즘을 통해 모델은 입력 문장의 단어에 다른 가중치를 할당하여 현재 출력 단어 생성에 대한 중요도를 반영할 수 있습니다.


    예를 들어, 모델이 "사랑"에 대한 프랑스어 단어를 생성할 때 주의 메커니즘은 입력 문장에서 "사랑"이라는 단어와 "나"라는 단어에 더 높은 가중치를 할당하여 이러한 단어가 해당 단어를 생성하는 데 중요함을 나타냅니다. 번역. 이를 통해 모델은 예측할 때 이러한 단어에 "참석"하거나 더 집중할 수 있습니다.


    마찬가지로 "개"에 대한 프랑스어 단어를 생성할 때 어텐션 메커니즘은 "개"라는 단어와 "I"라는 단어에 더 높은 가중치를 할당하여 올바른 번역을 생성하는 관련성을 나타냅니다.


    입력 시퀀스의 각 단어에 할당된 주의 가중치를 고려하여 모델은 현재 출력 단어를 생성하는 데 필요한 관련 정보를 캡처하는 인코더 출력의 가중치 조합을 생성할 수 있습니다. "컨텍스트 벡터"라고 하는 이 가중 조합은 각 디코딩 단계에서 입력 시퀀스의 컨텍스트별 표현을 디코더에 제공하여 더 정확한 예측을 할 수 있도록 도와줍니다.


    요약하면 어텐션 메커니즘을 통해 모델은 각 디코딩 단계에서 입력 시퀀스의 다른 부분에 초점을 맞추고 현재 컨텍스트에 따라 어텐션을 동적으로 조정할 수 있습니다. 이를 통해 모델은 단어 간의 관련 정보 및 종속성을 캡처하여 생성된 번역 또는 출력의 품질을 향상할 수 있습니다.



