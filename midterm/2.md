# Foundational Components of Neural Networks

# Learning from Data
    정의를 내리는것이 아님
    data를 기반으로 학습하는것
        ex-나무에 대한 정의는 잘 몰라도 이것을 보고 나무라는것을 판단
    명확한 solution이 있는게 아닌, 경험적 solution을 사용

    1. data
    2. 패턴
    3. 수학적 solution이 없을때 사용

#### ex)credit 승인
| ![](./img/20.PNG) | ![](./img/21.PNG) |
| ----------------- | ----------------- |
- threshold를 기준으로 approval, deny결정
- threshold를 기반으로한 decision boundary를 찾는것이 목표

# Model(Perceptron) - 인공신경망
#### ex)credit 승인
    인공신경망의 한 종류, 이진분류를 주로 풀기 위함
    위의 그림에서는 w1x1+w2x2+b=0 (b는 bias - 편차)
    이러한 모델(함수)를 perceptron이라고 함
| ![](./img/22.PNG) | ![](./img/23.PNG) |![](./img/24.PNG) |
| ----------------- | ----------------- | ----------------- |

# Activation Function( 위 사진에서의 neuron의 output)

### 1.Simoid(C)
    0~1사이의 값(확률적 변환이 필요할때)
    f(x)=1/1+e^(-x)
    torch.sigmoid()
    기울기가 0인 부분 -> vanishing gradient problem
        가중치가 제대로 적용되지 않을 수 있는 문제임
![](./img/25.PNG) 

### 2. ReLU*LeakyRelu(C)
    끝이 정해지지 않음(확률 x)
    입력값이 0보다 작을 때는 0을, 0보다 큰 경우는 그 값을 그대로 출력
    주로 layer사이는 relu, layer의 끝은 sigmod
    위의 gradient vanishing문제를 해결
![](./img/26.PNG) 

### 3.SoftMax(C)
    multiple case에 사용 <->sigmoid는 t/f와 같은 느낌
    k개의 가능한 class로 분류
    아래의 예시를 보면 softmax이후 y값의 합은 항상 1로 동일
| ![](./img/29.PNG) | ![](./img/28.PNG) |
| ----------------- | ----------------- |

# Loss Function
    prediction과 실제 target과의 비교
    낮은 값일수록 modeling이 잘되었고 예측능력이 좋다는 의미

### 1.Mean Squared Error Loss
    가장 흔한 방법
    오차에 대한 평균값을 구하는것
![](./img/30.PNG) 

### 2. Binary Cross-Entropy Loss
    확률 밀도 함수
### 3.Categorical Cross Entropy Loss
    다중 classification에서 사용
    다음과 같이 0.94인데 0 이면 예측값(y_pred)와 y가 차이가 많이남으로 loss가 크다
![](./img/31.PNG)


# Tranining

### Foward & Back Propagation
    Foward
        input signal
        network를 통해 propagated
        ouptput으로 병합
    backward
        원래는 neuron의 ouput
        뒤로 propagate
        forward에서 나온 error(loss function)을 줄이기 위해 => update or train
        주로 weights를 업데이트한다=>그럼 update방식은??
![](./img/32.PNG)

<hr>

### 1.Gradient Descent(backward의 방식)
    loss를 최소화 하는 알고리즘
    loss f의 미분값을 이용하여 오차줄이기는 방법
        현재 기울기의 반대 방향으로 업데이트
        손실함수 값이 작아지는 방향으로 학습
    데이터가 많아지만 해당 방법은 비효율적 -> 모든 데이터에 대해 decreasing loss를 해야해서
![](./img/33.PNG)

<hr>

### 2.Stochastic Gradient Descent
    위의 방법과 어느정도는 동일
    하지만 위의 방법의 문제를 해결
        => 랜덤한 data를 뽑는다(one data point) 

### 3.Mini-Batch Gradient Descent
    랜덤한 데이터를 2번방법과 다르게 batch형태로 뽑는다(약 100개정도?)

# Dive into Superviesd Training with Neural Network
![](./img/34.PNG)
- get_toy_data로 데이터 얻기
- model은 Perceptron사용
- Loss Function은 nn.BCELoss() 사용
- Optimizer는 Adam 사용
- backward를 통해 loss f 최소화 하도록 학습





