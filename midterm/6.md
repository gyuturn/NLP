# Intermediate Sequence Modeling for NLP

# Language Modeling
    문장에 대해 probability를 할당
        ex) P(pair|apple and) = 0.012=> apple and 뒤에 pair가 나올 확률
## RNN based Language Modeling
    train data: text corpus
    1. tokeization
        input sentence-> 각각의 token으로 변경
            the -> y1, pen -> y2 ....
        specail case
            - start sentence: <SOS>
            - end of sentence: <EOS>
            - unknown word: <UNK>
        2. supervised traning
            RNN model을 사용하여 token 확률을 모델링
            real output: one hot vector
            model output: softmax vector
|![](./img/78.png)| ![](./img/79.png)|
|---|---|
- y^: softmax vector(probability를 가짐) => ex) the: 0.4 a:0.3, child: 0.1....
-  y: 실제 값 
=> L(y^,y)를 통해 compute loss
- train과정에서는 이전의 ouput이 다음 input

# Various RNN aRchitecture and Beyond

## Bidirectional RNN
    RNN은 원래 과거의 데이터로부터만 학습됨
    하지만 whole sentence가 필요할때도 있음
    h: forward propagate-> 과거에 기반
    g: backward propagate -> 미래의 기반
![](./img/80.png)

## Deep RNN
    hidden recurrent state를 계층적으로 나눌 수 있음
    RNN에서 hidden state의 크기를 늘리는 대신, 여러 개의 RNN layer를 쌓아서 더 복잡한 모델을 만드는 방법입니다.
![](./img/81.png)

## Gated Recurrent Neural Networks

